# -*- coding: utf-8 -*-
"""BERT_transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17aYtA9EQqHSpEG-3leRAspDnEWUoLJfM
"""

!pip install transformers torch pandas
!pip install shap
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import numpy as np
import scipy as sp
import shap
import re
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from torch.optim import AdamW
from torch.cuda.amp import autocast, GradScaler
from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup, BertConfig
import itertools
from tqdm.auto import tqdm

raw = pd.read_csv('dataset_raw.csv')
no_html = pd.read_csv('dataset_no_html.csv')
normalized = pd.read_csv('dataset_normalized.csv')
lemmatized = pd.read_csv('dataset_lemmatized.csv')
corrected = pd.read_csv('dataset_corrected.csv')

def split(df, random_state=42, shuffle=True):

    treino, temp = train_test_split(
        df,
        test_size=0.2,
        random_state=random_state,
        shuffle=shuffle
    )

    validacao, teste = train_test_split(
        temp,
        test_size=0.5,
        random_state=random_state,
        shuffle=shuffle
    )

    return treino, validacao, teste

def prepare_data(df):
    encoding = tokenizer(
        df['text'].tolist(),
        add_special_tokens=True,
        max_length=200,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )
    labels = torch.tensor(df['label'].tolist())
    return encoding['input_ids'], encoding['attention_mask'], labels

datasets = (raw, no_html, normalized, lemmatized, corrected)
datasets_names = ('raw', 'no_html', 'normalized', 'lemmatized', 'corrected')

model_accuracies = {}

for name, dataset in zip(datasets_names, datasets):

    print(f"\nTraining on dataset: {name}")

    train, val, test = split(dataset)

    BATCH_SIZE = 64
    MODEL_NAME = 'bert-base-uncased'
    MAX_LEN = 200

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)

    train_ids, train_masks, train_labels = prepare_data(train)
    val_ids, val_masks, val_labels = prepare_data(val)

    train_data = TensorDataset(train_ids, train_masks, train_labels)
    val_data = TensorDataset(val_ids, val_masks, val_labels)

    HYPERPARAMS = {
        'batch_size': 64,
        'learning_rate': 2e-5,
        'epochs': 2,
        'dropout': 0.1
    }

    NUM_WORKERS = 4
    PIN_MEMORY = True if torch.cuda.is_available() else False

    train_loader = DataLoader(
        train_data,
        sampler=RandomSampler(train_data),
        batch_size=HYPERPARAMS['batch_size'],
        num_workers=NUM_WORKERS,
        pin_memory=PIN_MEMORY
    )

    val_loader = DataLoader(
        val_data,
        sampler=SequentialSampler(val_data),
        batch_size=HYPERPARAMS['batch_size'],
        num_workers=NUM_WORKERS,
        pin_memory=PIN_MEMORY
    )

    config = BertConfig.from_pretrained(
        MODEL_NAME,
        num_labels=2,
        hidden_dropout_prob=HYPERPARAMS['dropout'],
        attention_probs_dropout_prob=HYPERPARAMS['dropout']
    )

    final_model = BertForSequenceClassification.from_pretrained(MODEL_NAME, config=config)
    final_model.to(device)

    optimizer = AdamW(final_model.parameters(), lr=HYPERPARAMS['learning_rate'], eps=1e-8)
    total_steps = len(train_loader) * HYPERPARAMS['epochs']
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)
    scaler = GradScaler()

    # --- LOOP DE TREINO ---
    for epoch in range(HYPERPARAMS['epochs']):
        print(f"\nDataset: {name} | Epoch {epoch + 1} / {HYPERPARAMS['epochs']}")

        final_model.train()
        total_train_loss = 0

        train_pbar = tqdm(train_loader, desc=f"Treinando ({name})", unit="batch")

        for batch in train_pbar:
            b_ids = batch[0].to(device, non_blocking=True)
            b_mask = batch[1].to(device, non_blocking=True)
            b_labels = batch[2].to(device, non_blocking=True)

            final_model.zero_grad()

            with autocast():
                outputs = final_model(b_ids, attention_mask=b_mask, labels=b_labels)
                loss = outputs.loss

            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(final_model.parameters(), 1.0)
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()

            total_train_loss += loss.item()
            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})

        # --- VALIDAÇÃO ---
        final_model.eval()
        total_eval_accuracy = 0

        val_pbar = tqdm(val_loader, desc=f"Validando ({name})", unit="batch")

        for batch in val_pbar:
            b_ids = batch[0].to(device, non_blocking=True)
            b_mask = batch[1].to(device, non_blocking=True)
            b_labels = batch[2].to(device, non_blocking=True)

            with torch.no_grad():
                outputs = final_model(b_ids, token_type_ids=None, attention_mask=b_mask)

            logits = outputs.logits
            preds = torch.argmax(logits, dim=1).flatten()
            total_eval_accuracy += (preds == b_labels).cpu().numpy().sum()

        avg_val_accuracy = total_eval_accuracy / len(val_data)
        print(f"  Acurácia Validação ({name}): {avg_val_accuracy:.4f}")

    model_accuracies[name] = avg_val_accuracy

    print(f"Accuracy for '{name}' saved!")
    print("Current Model Accuracies:", model_accuracies)

print(model_accuracies)

def plotar_comparacao_modelos(model_accuracies):
    names = list(model_accuracies.keys())
    values = list(model_accuracies.values())

    plt.figure(figsize=(10, 6))

    # Criando o gráfico de barras verticais
    # color='cornflowerblue' é uma cor agradável, mas pode mudar para 'gray', 'red', etc.
    bars = plt.bar(names, values, color='cornflowerblue', edgecolor='black', width=0.6)

    plt.title('Comparação de Acurácia dos Modelos', fontsize=14)
    plt.ylabel('Acurácia', fontsize=12)
    plt.xlabel('Modelos', fontsize=12)

    # Define o limite do eixo Y um pouco acima de 1.0 (100%) para caber o texto
    plt.ylim(0, 1.1)

    # Adiciona o valor percentual exato em cima de cada barra
    for bar in bars:
        height = bar.get_height()
        plt.text(
            bar.get_x() + bar.get_width() / 2.0, # Posição X (centro da barra)
            height + 0.01,                       # Posição Y (um pouco acima da barra)
            f'{height:.2%}',                     # Texto formatado como porcentagem (ex: 85.00%)
            ha='center', va='bottom', fontsize=11, fontweight='bold'
        )

    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

plotar_comparacao_modelos(model_accuracies)

train, val, test = split(normalized) #Normalized foi o que obteve a maior acurácia, apesar de pouca diferença.

BATCH_SIZE = 64
MODEL_NAME = 'bert-base-uncased' # Inglês
MAX_LEN = 200
NUM_WORKERS = 4
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
PIN_MEMORY = True if torch.cuda.is_available() else False
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)

train_ids, train_masks, train_labels = prepare_data(train)
val_ids, val_masks, val_labels = prepare_data(val)
test_ids, test_masks, test_labels = prepare_data(test)

# DataLoaders
train_data = TensorDataset(train_ids, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)

val_data = TensorDataset(val_ids, val_masks, val_labels)
val_loader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=BATCH_SIZE)

test_data = TensorDataset(test_ids, test_masks, test_labels)
test_loader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=BATCH_SIZE)

# --- CONFIGURAÇÕES DE HARDWARE ---
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MODEL_NAME = 'bert-base-uncased'

# Otimizações Fixas
NUM_WORKERS = 4
PIN_MEMORY = True
FIXED_BATCH_SIZE = 64

# --- A ARENA: PARÂMETROS PARA TESTAR ---
param_grid = {
    'learning_rate': [2e-5, 3e-5],
    'dropout': [0.1, 0.3],
    'epochs': [2, 3]
}

# Gera todas as combinações
keys, values = zip(*param_grid.items())
combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]

print(f"Total de combinações para testar: {len(combinations)}")
print(f"Configuração: Batch Size {FIXED_BATCH_SIZE} | Workers {NUM_WORKERS}")

# --- LOOP DO GRID SEARCH ---

best_accuracy = 0.0
best_params = {}
best_model_state = None

# Dataloaders (fixos)
train_loader = DataLoader(
    train_data,
    sampler=RandomSampler(train_data),
    batch_size=FIXED_BATCH_SIZE,
    num_workers=NUM_WORKERS,
    pin_memory=PIN_MEMORY
)

val_loader = DataLoader(
    val_data,
    sampler=SequentialSampler(val_data),
    batch_size=FIXED_BATCH_SIZE,
    num_workers=NUM_WORKERS,
    pin_memory=PIN_MEMORY
)

for i, params in enumerate(combinations):
    run_id = f"Run {i+1}/{len(combinations)}"
    print(f"\n>>> {run_id} | Params: {params}")

    # 1. Configuração Personalizada do Modelo
    config = BertConfig.from_pretrained(
        MODEL_NAME,
        num_labels=2,
        output_attentions=False,
        output_hidden_states=False,
        hidden_dropout_prob=params['dropout'],
        attention_probs_dropout_prob=params['dropout']
    )

    # 2. Carregar Modelo
    model = BertForSequenceClassification.from_pretrained(MODEL_NAME, config=config)
    model.to(device)

    # 3. Otimizador e Scheduler
    optimizer = AdamW(model.parameters(), lr=params['learning_rate'], eps=1e-8)
    total_steps = len(train_loader) * params['epochs']
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)
    scaler = GradScaler()

    # 4. Loop de Épocas
    for epoch in range(params['epochs']):

        # --- TREINO ---
        model.train()

        train_pbar = tqdm(train_loader, desc=f"   Ep {epoch+1} Treino", leave=False)

        for batch in train_pbar:
            b_ids = batch[0].to(device, non_blocking=True)
            b_mask = batch[1].to(device, non_blocking=True)
            b_labels = batch[2].to(device, non_blocking=True)

            model.zero_grad()

            with autocast():
                outputs = model(b_ids, attention_mask=b_mask, labels=b_labels)
                loss = outputs.loss

            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()

            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})

        # --- VALIDAÇÃO ---
        model.eval()
        total_eval_accuracy = 0

        for batch in val_loader:
            b_ids = batch[0].to(device, non_blocking=True)
            b_mask = batch[1].to(device, non_blocking=True)
            b_labels = batch[2].to(device, non_blocking=True)

            with torch.no_grad():
                with autocast():
                    outputs = model(b_ids, token_type_ids=None, attention_mask=b_mask)

            logits = outputs.logits
            preds = torch.argmax(logits, dim=1).flatten()
            total_eval_accuracy += (preds == b_labels).cpu().numpy().sum()

        avg_val_accuracy = total_eval_accuracy / len(val_data)


        if avg_val_accuracy > best_accuracy:
            print(f"   ★ Novo Recorde: {avg_val_accuracy:.4f} (Antigo: {best_accuracy:.4f})")
            best_accuracy = avg_val_accuracy
            best_params = params


            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
        else:
            print(f"   - Ep {epoch+1} Acc: {avg_val_accuracy:.4f}")

    del model
    del optimizer
    del scheduler
    del scaler
    torch.cuda.empty_cache()
    gc.collect()

# --- FIM DO GRID SEARCH ---
print("\n" + "="*40)
print(f"MELHOR RESULTADO GLOBAL: {best_accuracy:.4f}")
print(f"MELHORES PARÂMETROS: {best_params}")
print("="*40)

# --- RECARREGAR O CAMPEÃO ---
print("Restaurando o melhor modelo da memória RAM...")

final_config = BertConfig.from_pretrained(
    MODEL_NAME,
    num_labels=2,
    hidden_dropout_prob=best_params['dropout'],
    attention_probs_dropout_prob=best_params['dropout']
)

final_model = BertForSequenceClassification.from_pretrained(MODEL_NAME, config=final_config)
final_model.load_state_dict(best_model_state)
final_model.to(device)

print("Modelo final pronto para testes!")

from sklearn.metrics import classification_report, roc_auc_score
import torch.nn.functional as F
import numpy as np

# 1. Configuração do DataLoader (Otimizado)
test_loader = DataLoader(
    test_data,
    sampler=SequentialSampler(test_data),
    batch_size=16,
    num_workers=NUM_WORKERS,
    pin_memory=PIN_MEMORY
)

# 2. Preparação
final_model.eval()

true_labels = []
pred_discrete = [] # Para o Classification Report (0 ou 1)
pred_probs = []    # Para a AUC-ROC (0.0 a 1.0)

print("Iniciando avaliação completa (Report + AUC)...")

# 3. Loop de Inferência
for batch in test_loader:
    b_input_ids = batch[0].to(device, non_blocking=True)
    b_input_mask = batch[1].to(device, non_blocking=True)
    b_labels = batch[2].to(device, non_blocking=True)

    with torch.no_grad():
        with autocast():
            outputs = final_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)

    logits = outputs.logits

    # --- PROCESSAMENTO PARA MÉTRICAS ---

    # A. Para AUC-ROC: Precisamos das probabilidades
    probs = F.softmax(logits, dim=1)
    probs_positive = probs[:, 1].cpu().numpy() # Probabilidade da classe 1
    pred_probs.extend(probs_positive)

    # B. Para Classification Report: Precisamos da classe final (0 ou 1)
    preds = torch.argmax(logits, dim=1).cpu().numpy()
    pred_discrete.extend(preds)

    # C. Labels Reais
    label_ids = b_labels.cpu().numpy()
    true_labels.extend(label_ids)

print("Avaliação concluída.\n")

# --- 4. EXIBIÇÃO DOS RESULTADOS ---

# Cálculo da AUC-ROC
try:
    auc_score = roc_auc_score(true_labels, pred_probs)
    print(f"AUC-ROC Score: {auc_score:.5f}")
except ValueError as e:
    print(f"Erro ao calcular AUC: {e}")

print("\nRelatório de Classificação:")
# target_names ajuda a identificar quem é 0 e quem é 1 no relatório
print(classification_report(true_labels, pred_discrete, target_names=['Negativo', 'Positivo']))

# ==========================================
# 1. CONFIGURAÇÃO E AMOSTRAGEM
# ==========================================
final_model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Função wrapper (igual às anteriores)
def f(textos):
    if isinstance(textos, str): textos = [textos]
    inputs = tokenizer(
        list(textos),
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    ).to(device)
    with torch.no_grad():
        outputs = final_model(**inputs)
    scores = torch.nn.functional.softmax(outputs.logits, dim=1).detach().cpu().numpy()
    return scores

# Pegamos 50 reviews aleatórios para a análise global
# (Se tiver GPU boa, pode aumentar para 100 ou 200)
n_samples = 50
print(f"Selecionando {n_samples} reviews aleatórios...")
amostra = test.sample(n=n_samples, random_state=42)
textos_amostra = amostra['text'].tolist()

# ==========================================
# 2. CÁLCULO DO SHAP
# ==========================================
masker = shap.maskers.Text(tokenizer)
explainer = shap.Explainer(f, masker, output_names=["Negativo", "Positivo"])

print("Calculando SHAP Values (Isso pode levar 1 ou 2 minutos)...")
shap_values = explainer(textos_amostra)

# ==========================================
# 3. PROCESSAMENTO DOS DADOS (AGREGAÇÃO)
# ==========================================
print("Processando dados para análise global...")

all_words = []
all_values = []

# Varre cada review e "desmonta" as listas para criar um tabelão único
for i in range(len(shap_values)):
    # Pega as palavras e os valores da classe POSITIVA (índice 1)
    words = shap_values[i].data
    values = shap_values[i].values[:, 1] # Valores positivos puxam pra cima, negativos pra baixo

    # Adiciona na lista geral (limpando espaços em branco)
    for w, v in zip(words, values):
        if w.strip(): # Ignora strings vazias ou só espaços
            all_words.append(w.strip()) # .strip() limpa espaços extras
            all_values.append(v)

# Cria um DataFrame gigante com todas as palavras encontradas
df_global = pd.DataFrame({'word': all_words, 'shap_value': all_values})

# Agrupa por palavra para tirar a média de impacto
# Filtramos palavras que apareceram pelo menos 3 vezes para evitar ruído estatístico
word_stats = df_global.groupby('word').agg(['mean', 'count'])
word_stats.columns = ['mean_shap', 'count']
word_stats = word_stats[word_stats['count'] >= 3] # Mínimo de aparições

# Separa os campeões
top_positive = word_stats.sort_values(by='mean_shap', ascending=False).head(20)
top_negative = word_stats.sort_values(by='mean_shap', ascending=True).head(20)

# ==========================================
# 4. PLOTAGEM DOS GRÁFICOS
# ==========================================
fig, axes = plt.subplots(1, 2, figsize=(16, 8))

# Gráfico 1: As palavras mais NEGATIVAS (Azul)
sns.barplot(x=top_negative['mean_shap'], y=top_negative.index, ax=axes[0], color="#008bfb")
axes[0].set_title(f"Top 20 Palavras que indicam sentimento NEGATIVO")
axes[0].set_xlabel("Impacto Médio (SHAP Value)")
axes[0].axvline(0, color='black', linewidth=0.5)

# Gráfico 2: As palavras mais POSITIVAS (Vermelho)
sns.barplot(x=top_positive['mean_shap'], y=top_positive.index, ax=axes[1], color="#ff0051")
axes[1].set_title(f"Top 20 Palavras que indicam sentimento POSITIVO")
axes[1].set_xlabel("Impacto Médio (SHAP Value)")
axes[1].axvline(0, color='black', linewidth=0.5)

plt.tight_layout()
plt.show()

# Mostra tabelinha também
print("\n--- AMOSTRA DAS PALAVRAS MAIS FORTES ---")
print(top_positive.head(5))
print("\n--- AMOSTRA DAS PALAVRAS MAIS FRACAS (NEGATIVAS) ---")
print(top_negative.head(5))